\clearpage



\begin{appendix}
\section{}
SSBH make a number of factual misstatements and mathematical errors.

In Table 2 and surrounding text, they report some discrepancies betweern
the number of subjects per condition for the critical analyses reported
by HTP (p.~266) and in SSBH's own analyses. The problem seems to be that
they ran their exclusions in a different order from HTP, despite basing
their analyses on HTP's code. Specifically, both papers bin subjects by
age, age of acquisition, and condition. We then restrict analyses to
consecutive ages for which there were at least 10 parrticipants in a
5-year window. HTP excludes subjects over the age of 70 before this
binning, whereas SSBH exclude subjects over the age of 70 \emph{after}
binning. This means subjects over teh age of 70 count towards binning
for SSBH but not for HTP, allowing inclusion of more bins for SSBH.
Thus, as they report, they end up with 38 more total included subjects.

SSBH report that HTP defined immersion learners as either simultaneous
bilinguals or ``later learners who spent at least 90\% of their life in
an English-speaking country'' (SSBH, p.~7). In fact, later immersion
learners were required to have spent at least 90\% of their life
\emph{since starting to learn English} in an English speaking country
(HTP, p.~266). This error is a bit of a head-scratcher: analyses include
immersion learners who began learning English as late as 30, which would
require them to be at least 300 years old at time of testing. Similarly,
they mistakenly report that non-immersion learners were those ``who
spent at most 10\% of their life in an English-speaking country'' (SSBH,
p.~8), whereas the actual definition is ``spent at most 10\% of
post-exposure life in an English-speaking country and no more than 1
year in total'' (HTP, p.~266).

Probably because of their confusion about how subject groups were
defined, SSBH mistakenly report that ``more than 100,000 language
learners in the HTP database could not be classified as belonging to one
of the four groups \emph{because key information was missing}''
(emphasis added; p.~20). They assert that this high rate of missing data
should cast doubt on the validity/accuracy of the HTP data. However,
these subjects were not excluded for missing data but rather for having
amounts of immersion intermediate between the ``immersion'' and
``non-immersion'' learners (see sentence spanning pp.~266-267).

SSBH misdescribe the stimuli. They report that HTP's test included 132
items, of which 95 were used for analysis ``based on the criterion that
at least 70\% of the native English-speaking adults gave the same
response'' (SSBH, p.~8). In fact, the criterion was that the same
response was given by at least 70\% of native English-speaking adults in
each of 13 dialect groups (HTP, p.~267). The reason was to exclude items
for which there was significant dialectal variation. They also assert
that HTP measures accuracy on the grammaticality judgment test on a
scale of 0 to 1, reflecting ``a proportion of correct answers (g)''
(SSBH, p.~7). In fact, \emph{g} represents log-odds accuracy on HTP's
syntax test and runs from 1.5 to 3.5 (see HTP Supplementary Materials,
p.~2). They misstate how HTP (and, it appears, they) calculated
log-odds, asserting that it was based on proportion
(\emph{log(p/{[}1-p{]})}) (SSBH, p.~7) rather than the empirical logit
transformation (\emph{log((ncorrect+.5)/(nincorrect+.5))}).

SSBH use Akaike Information Criterion (AIC) for model comparison, but in
almost every case appear to have miscounted the number of parameters in
the models (a key part of calculating AIC). For most of their analyses,
the ``continuous'' model has 4 free parameters (\(r_0\), \(\alpha\),
\(\delta\), and the error variance), though in all but one case, they
count it as having 5. The ``discontinuous'' model has one additional
free parameter (\(t_c\)) but for some reason is counted as having 7. The
exceptions are as follows: In the case of the monolingual analysis, they
correctly assign the continuous model 4 parameters, but again over-count
the discontinuous model (6 instead of 5). When fit to all data, there
are 3 additional parameters (the three E parameters), which should give
the ``continuous'' model 7 parameters (which they code correctly) and
give the ``discontinuous'' model 8 (they count 9).

(Note that they explain in Footnote 5 that ``the discontinuous model
needs to fit three components, the continuous model only one (cf.~Figure
1). That explains the difference of two degrees of freedom.'' It is not
possible to count degrees of freedom by inspecting a graph, and the
numbers here do not match the numbers in their code.)

These errors tend to overstate the evidence for the ``continuous''
model. For instance, the relative likelihood for the monolingual
analyses in their Table 3 is reported as 0.16. Using the correct number
of parameters, it is 0.30. That is, using AIC correctly, rather than the
``continuous'' model being nearly 7 times more likely, it is only about
3 times more likely. (Strangely, using SSBH's counting of parameters,
the ratio is actually 0.08; I have not yet identified the source of that
error.)

When replicating one of HTP's analyses, they report that they obtained
``a slightly higher \(R^2\) value of .92 (HTP found .89)'' (SSBH,
p.~10). This likely reflects the fact that while HTP report
cross-validated \(R^2\) values in order to address over-fitting, SSBH do
not. This will necessarily result in higher \(R^2\) values. It is not
clear whether this was an oversight or a misunderstanding of how
curve-fitting works. In a personal communication, van der Slik suggested
that because they ran the optimization algorithm for more iterations
than did HTP, this should obviate the need for cross-validation. This is
exactly backwards. It is a necessary fact that the more closely the
model is fit to the data, the worse over-fitting gets. In any case, the
result is that the \(R^2\) values must be treated with caution: a
particular model may achieve a better \(R^2\) simply due to overfitting.

In Footnote 7, they write that Chen and Hartshorne ``did not test if the
application of their segmented model has resulted in a significant
improvement in model fit as compared to the continuous model or even the
original HTP discontinuous model.'' In fact, we provided two such
metrics. First, the model fits available to ELSD are a proper subset of
those available to Chen \& Hartshorne's segmented sigmoid model, and
thus fitting the revised model is \emph{per se} a comparison of model
fit. Second, Chen and Hartshorne also provide cross-validated R\^{}2
statistics for both models, which corrects for model complexity.

Note that this list is probably not complete. In particular, I did not
check all (or even most) of their calculations. I focused on those
critical the present discussion or which contained easy-to-spot
mistakes.
\end{appendix}
